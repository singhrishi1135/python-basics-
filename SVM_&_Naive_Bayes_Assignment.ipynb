{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1:** What is a Support Vector Machine (SVM), and how does it work?"
      ],
      "metadata": {
        "id": "6MevyQ3-xU14"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:** A Support Vector Machine (SVM) is a powerful supervised machine learning algorithm used primarily for classification tasks, although it can also be used for regression. The main idea behind SVM is to find the optimal hyperplane that best separates data points of different classes in a high-dimensional space.\n",
        "\n",
        "**How SVM Works:**\n",
        "\n",
        "**1. Linear Separation:**\n",
        "\n",
        "- SVM tries to find a straight line (in 2D), a plane (in 3D), or a hyperplane (in higher dimensions) that separates the data points of different classes with the maximum margin.\n",
        "\n",
        "- The margin is the distance between the hyperplane and the closest data points from each class. These closest points are called support vectors.\n",
        "\n",
        "**Optimal Hyperplane:**\n",
        "\n",
        "- The optimal hyperplane is the one that maximizes the margin between the support vectors of the two classes.\n",
        "\n",
        "**Non-Linearly Separable Data:**\n",
        "\n",
        "When the data isn't linearly separable, SVM uses a technique called the kernel trick to transform the data into a higher-dimensional space where a linear separator might exist.\n",
        "\n",
        "**- Common kernels:**\n",
        "\n",
        "- Linear\n",
        "\n",
        "- Polynomial\n",
        "\n",
        "- Radial Basis Function (RBF) or Gaussian\n",
        "\n",
        "- Sigmoid\n",
        "\n",
        "**Soft Margin (for overlapping classes):**\n",
        "\n",
        "- SVM allows some misclassification using a soft margin and introduces a parameter C to control the trade-off between maximizing the margin and minimizing classification errors.\n",
        "\n",
        "**Mathematical Insight:**\n",
        "**- For linearly separable data:**\n",
        "\n",
        "- The hyperplane is defined by:\n",
        "\n",
        "**w⋅x+b=0**\n",
        "\n",
        "Maximize the margin:\n",
        "2/\n",
        "∥\n",
        "𝑤\n",
        "∥\n",
        "\n",
        "\n",
        "- Subject to the constraint for each data point:\n",
        " y\n",
        "i\n",
        "​\n",
        " (w⋅x\n",
        "i\n",
        "​\n",
        " +b)≥1\n",
        "\n",
        "**Advantages of SVM:**\n",
        "\n",
        "- Works well with high-dimensional data.\n",
        "\n",
        "- Effective when the number of features > number of samples.\n",
        "\n",
        "- Can model non-linear decision boundaries using kernels.\n",
        "\n",
        "- Robust to overfitting (especially with proper kernel and C tuning).\n",
        "\n"
      ],
      "metadata": {
        "id": "c-6ftlxPxjzf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 2:** Explain the difference between Hard Margin and Soft Margin SVM."
      ],
      "metadata": {
        "id": "5KWNbTJqz6R4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:**  The difference between Hard Margin and Soft Margin SVM lies in how strictly the algorithm enforces the separation of data points when creating the decision boundary (hyperplane).\n",
        "\n",
        "**1. Hard Margin SVM:**\n",
        "\n",
        "- Hard Margin SVM strictly separates the data.\n",
        "\n",
        "- It assumes the data is linearly separable, meaning there exists a clear margin between the two classes with no overlap or misclassification.\n",
        "\n",
        "**How it works:**\n",
        "- Finds a hyperplane that perfectly separates the data with the maximum margin.\n",
        "\n",
        "- No tolerance for misclassified data points.\n",
        "\n",
        "**Mathematical Constraint:**\n",
        "\n",
        "y\n",
        "i\n",
        "​\n",
        " (w⋅x\n",
        "i\n",
        "​\n",
        " +b)≥1\n",
        "\n",
        " for all\n",
        "𝑖\n",
        "\n",
        "**Limitation:**\n",
        "- Highly sensitive to outliers.\n",
        "\n",
        "- Not suitable for real-world noisy data.\n",
        "\n",
        "**2. Soft Margin SVM:**\n",
        "\n",
        "**Definition:**\n",
        "- Soft Margin SVM allows some misclassification of data points to achieve better generalization.\n",
        "\n",
        "- Designed to handle non-linearly separable or noisy data.\n",
        "\n",
        "**How it works:**\n",
        "- Introduces slack variables (ξᵢ) that allow some violations of the margin constraints.\n",
        "\n",
        "- Includes a regularization parameter\n",
        "𝐶\n",
        "C that balances margin width and classification error:\n",
        "\n",
        "- Large\n",
        "𝐶\n",
        "C: Less tolerance for misclassification (tries to fit the training data closely).\n",
        "\n",
        "- Small\n",
        "𝐶\n",
        "C: More tolerance, better generalization.\n",
        "\n",
        "**Mathematical Constraint:**\n",
        "\n",
        "y\n",
        "i\n",
        "​\n",
        " (w⋅x\n",
        "i\n",
        "​\n",
        " +b)≥1−ξ\n",
        "i\n",
        "​\n",
        " ,ξ\n",
        "i\n",
        "​\n",
        " ≥0\n",
        "\n",
        "**Key Differences Table:**\n",
        "\n",
        "| Feature              | Hard Margin SVM              | Soft Margin SVM                       |\n",
        "| -------------------- | ---------------------------- | ------------------------------------- |\n",
        "| Data Requirement     | Perfectly linearly separable | Works with non-separable, noisy data  |\n",
        "| Misclassification    | Not allowed                  | Allowed (with penalty)                |\n",
        "| Flexibility          | Rigid                        | Flexible                              |\n",
        "| Sensitivity to Noise | Very high                    | Lower                                 |\n",
        "| Use of Slack (ξᵢ)    | No                           | Yes                                   |\n",
        "| Regularization (C)   | Not used                     | Used to control margin–error tradeoff |\n"
      ],
      "metadata": {
        "id": "GAuED3u-0Crn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 3:** What is the Kernel Trick in SVM? Give one example of a kernel and\n",
        "explain its use case.\n"
      ],
      "metadata": {
        "id": "Wkb-jcwt1Xk3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:** The Kernel Trick is a mathematical technique used in Support Vector Machines (SVM) to enable the algorithm to solve non-linearly separable problems by implicitly mapping data into a higher-dimensional space—without actually computing the coordinates in that space.\n",
        "\n",
        "This allows SVM to find a linear hyperplane in that higher-dimensional space, which corresponds to a non-linear decision boundary in the original space.\n",
        "\n",
        "**Need:**\n",
        "In many real-world problems, data cannot be separated by a straight line (linear boundary). The kernel trick helps by:\n",
        "\n",
        "- Transforming input features into a higher-dimensional space where a linear separator may exist.\n",
        "\n",
        "- Doing this transformation efficiently using kernel functions instead of explicitly calculating the mapping.\n",
        "\n",
        "SVM optimization involves the dot product of feature vectors:\n",
        "K(x\n",
        "i\n",
        "​\n",
        " ,x\n",
        "j\n",
        "​\n",
        " )=ϕ(x\n",
        "i\n",
        "​\n",
        " )⋅ϕ(x\n",
        "j\n",
        "​\n",
        " )\n",
        "\n",
        " The kernel function\n",
        "𝐾\n",
        "(\n",
        "𝑥\n",
        "𝑖\n",
        ",\n",
        "𝑥\n",
        "𝑗\n",
        ")\n",
        "K(x\n",
        "i\n",
        "​\n",
        " ,x\n",
        "j\n",
        "​\n",
        " ) computes this dot product directly in the high-dimensional space, avoiding the need to compute\n",
        "𝜙\n",
        "(\n",
        "𝑥\n",
        ")\n",
        "ϕ(x) explicitly.\n",
        "\n",
        "**Example of a Kernel Function:**\n",
        "\n",
        "Radial Basis Function (RBF) Kernel / Gaussian Kernel:\n",
        "\n",
        "K(x\n",
        "i\n",
        "​\n",
        " ,x\n",
        "j\n",
        "​\n",
        " )=exp(−γ∥x\n",
        "i\n",
        "​\n",
        " −x\n",
        "j\n",
        "​\n",
        " ∥\n",
        "2\n",
        " )\n",
        "\n",
        "\n",
        " γ: controls the influence of a single training example. Higher\n",
        "𝛾\n",
        "γ means closer points have more influence.\n",
        "\n",
        "**Use Case:**\n",
        "\n",
        "Image classification (e.g., handwritten digit recognition like MNIST dataset)\n",
        "\n",
        "- Data points (images) are complex and not linearly separable in raw pixel form.\n",
        "\n",
        "- RBF kernel maps them into a space where SVM can find a hyperplane that separates digits like ‘3’ and ‘8’ even when they have similar shapes.\n",
        "\n",
        "**Other Common Kernels:**\n",
        "\n",
        "| Kernel Type | Formula                                         | Use Case                                     |\n",
        "| ----------- | ----------------------------------------------- | -------------------------------------------- |\n",
        "| Linear      | $K(x_i, x_j) = x_i \\cdot x_j$                   | High-dimensional but linearly separable data |\n",
        "| Polynomial  | $K(x_i, x_j) = (x_i \\cdot x_j + c)^d$           | Text classification, NLP                     |\n",
        "| Sigmoid     | $K(x_i, x_j) = \\tanh(\\alpha x_i \\cdot x_j + c)$ | Neural network similarity                    |\n"
      ],
      "metadata": {
        "id": "sW7FyfEc1fmh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 4:** What is a Naïve Bayes Classifier, and why is it called “naïve”?"
      ],
      "metadata": {
        "id": "qLwPklF12b2f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:** The Naïve Bayes Classifier is a probabilistic machine learning algorithm based on Bayes' Theorem. It is primarily used for classification tasks, such as text classification, spam filtering, and sentiment analysis.\n",
        "\n",
        "It calculates the probability of a class given a set of features and chooses the class with the highest posterior probability.\n",
        "\n",
        "** Bayes’ Theorem Recap:**\n",
        "\n",
        "P(C∣X)=\n",
        "P(X)\n",
        "P(X∣C)⋅P(C)\n",
        "​\n",
        "Where:\n",
        "\n",
        "𝑃\n",
        "(\n",
        "𝐶\n",
        "∣\n",
        "𝑋\n",
        ")\n",
        "P(C∣X): Posterior probability (class given features)\n",
        "\n",
        "𝑃\n",
        "(\n",
        "𝑋\n",
        "∣\n",
        "𝐶\n",
        ")\n",
        "P(X∣C): Likelihood (features given class)\n",
        "\n",
        "𝑃\n",
        "(\n",
        "𝐶\n",
        ")\n",
        "P(C): Prior probability (class probability)\n",
        "\n",
        "𝑃\n",
        "(\n",
        "𝑋\n",
        ")\n",
        "P(X): Evidence (probability of features)\n",
        "\n",
        "It’s called naïve because it assumes all features are independent of each other given the class label, which is rarely true in real-world data.\n",
        "\n",
        "**Example:**\n",
        "In spam filtering, features might be the presence of words like \"free\", \"money\", and \"click\".\n",
        "\n",
        "- Naïve Bayes assumes the appearance of the word “free” is independent of the word “money” given the message is spam — which is obviously a simplifying assumption.\n",
        "\n",
        "Despite this unrealistic assumption, Naïve Bayes often works surprisingly well, especially in text-based applications.\n",
        "\n",
        "**Types of Naïve Bayes Classifiers:**\n",
        "\n",
        "| Type               | Feature Type          | Example Use Case    |\n",
        "| ------------------ | --------------------- | ------------------- |\n",
        "| **Gaussian NB**    | Continuous features   | Medical diagnosis   |\n",
        "| **Multinomial NB** | Discrete word counts  | Text classification |\n",
        "| **Bernoulli NB**   | Binary features (0/1) | Spam detection      |\n",
        "\n",
        "\n",
        "**Advantages:**\n",
        "\n",
        "- Very fast and scalable.\n",
        "\n",
        "- Works well with high-dimensional data (e.g., text).\n",
        "\n",
        "- Performs well even with relatively small datasets."
      ],
      "metadata": {
        "id": "Lz1fZJ4I2iuA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 5:** Describe the Gaussian, Multinomial, and Bernoulli Naïve Bayes variants.\n",
        "When would you use each one?\n"
      ],
      "metadata": {
        "id": "fST6oBXR3K0Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:** Naïve Bayes classifiers have three main variants, each suited to a different type of feature distribution in your data. The choice of variant depends on the nature of the input features.\n",
        "\n",
        "**1. Gaussian Naïve Bayes**\n",
        "- Assumes that the features follow a normal (Gaussian) distribution.\n",
        "\n",
        "- Typically used when features are continuous numerical values.\n",
        "\n",
        "*Formula*\n",
        "\n",
        "The likelihood\n",
        "𝑃\n",
        "(\n",
        "𝑥\n",
        "𝑖\n",
        "∣\n",
        "𝑦\n",
        ")\n",
        "P(x\n",
        "i\n",
        "​\n",
        " ∣y) is computed using the Gaussian (normal) distribution:\n",
        "\n",
        "𝑃\n",
        "(\n",
        "𝑥\n",
        "𝑖\n",
        "∣\n",
        "𝑦\n",
        ")\n",
        "=\n",
        "1\n",
        "2\n",
        "𝜋\n",
        "𝜎\n",
        "𝑦\n",
        "2\n",
        "exp\n",
        "⁡\n",
        "(\n",
        "−\n",
        "(\n",
        "𝑥\n",
        "𝑖\n",
        "−\n",
        "𝜇\n",
        "𝑦\n",
        ")\n",
        "2\n",
        "2\n",
        "𝜎\n",
        "𝑦\n",
        "2\n",
        ")\n",
        "P(x\n",
        "i\n",
        "​\n",
        " ∣y)=\n",
        "2πσ\n",
        "y\n",
        "2\n",
        "​\n",
        "\n",
        "​\n",
        "\n",
        "1\n",
        "​\n",
        " exp(−\n",
        "2σ\n",
        "y\n",
        "2\n",
        "​\n",
        "\n",
        "(x\n",
        "i\n",
        "​\n",
        " −μ\n",
        "y\n",
        "​\n",
        " )\n",
        "2\n",
        "\n",
        "​\n",
        " )\n",
        "\n",
        "\n",
        "** Use Case:**\n",
        "\n",
        "- Medical diagnosis (e.g., classifying diseases based on blood pressure, cholesterol, etc.)\n",
        "\n",
        "- Iris flower classification (based on petal/sepal lengths)\n",
        "\n",
        "- Sensor data classification.\n",
        "\n",
        "**2. Multinomial Naïve Bayes**\n",
        "\n",
        "**Description:**\n",
        "- Assumes features are discrete counts (e.g., word frequencies).\n",
        "\n",
        "- Used when data represents counts or frequencies, especially in document classification.\n",
        "\n",
        "**Formula:**\n",
        "The likelihood is based on the frequency of each feature (like word count) in a class:\n",
        "\n",
        "P(x\n",
        "i\n",
        "​\n",
        " ∣y)=\n",
        "total count of all features in class y+n\n",
        "count(x\n",
        "i\n",
        "​\n",
        "  in class y)+1\n",
        "​\n",
        "\n",
        "**Use Case:**\n",
        "Text classification, such as:\n",
        "\n",
        "- Spam detection\n",
        "\n",
        "- News categorization\n",
        "\n",
        "- Sentiment analysis\n",
        "\n",
        "- Works well with Bag of Words or TF-IDF feature representations\n",
        "\n",
        "**3. Bernoulli Naïve Bayes**\n",
        "\n",
        "- Assumes binary features (0 or 1), representing presence or absence of a feature.\n",
        "\n",
        "- Suitable when features are booleans (e.g., “Is the word ‘offer’ present in the email?”).\n",
        "\n",
        "Formula:\n",
        "Likelihoods are computed for binary outcomes:\n",
        "\n",
        "P(x\n",
        "i\n",
        "​\n",
        " =1∣y) and P(x\n",
        "i\n",
        "​\n",
        " =0∣y)\n",
        "\n",
        "** Summary Table:**\n",
        "\n",
        "| Variant            | Feature Type    | Distribution Assumed | Typical Use Case                        |\n",
        "| ------------------ | --------------- | -------------------- | --------------------------------------- |\n",
        "| **Gaussian NB**    | Continuous      | Normal (bell curve)  | Sensor data, medical stats              |\n",
        "| **Multinomial NB** | Discrete counts | Multinomial          | Word count-based text classification    |\n",
        "| **Bernoulli NB**   | Binary (0/1)    | Bernoulli (yes/no)   | Binary-feature models, keyword presence |\n"
      ],
      "metadata": {
        "id": "5W0QNX4F3_YQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer: **"
      ],
      "metadata": {
        "id": "7ehHhgmN6Pt3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load the Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data        # Features\n",
        "y = iris.target      # Labels\n",
        "\n",
        "# Step 2: Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Train an SVM Classifier with a linear kernel\n",
        "svm_model = SVC(kernel='linear')\n",
        "svm_model.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Make predictions on the test data\n",
        "y_pred = svm_model.predict(X_test)\n",
        "\n",
        "# Step 5: Calculate and print the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "# Step 6: Print the support vectors\n",
        "print(\"\\nSupport Vectors:\")\n",
        "print(svm_model.support_vectors_)\n",
        "\n",
        "# Optionally: Print indices of support vectors for each class\n",
        "print(\"\\nSupport Vector Indices for each class:\")\n",
        "print(svm_model.support_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MmDOM_KX6Rkw",
        "outputId": "52b407c6-6179-497a-a44c-37faf0d686a4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 100.00%\n",
            "\n",
            "Support Vectors:\n",
            "[[4.8 3.4 1.9 0.2]\n",
            " [5.1 3.3 1.7 0.5]\n",
            " [4.5 2.3 1.3 0.3]\n",
            " [5.6 3.  4.5 1.5]\n",
            " [5.4 3.  4.5 1.5]\n",
            " [6.7 3.  5.  1.7]\n",
            " [5.9 3.2 4.8 1.8]\n",
            " [5.1 2.5 3.  1.1]\n",
            " [6.  2.7 5.1 1.6]\n",
            " [6.3 2.5 4.9 1.5]\n",
            " [6.1 2.9 4.7 1.4]\n",
            " [6.5 2.8 4.6 1.5]\n",
            " [6.9 3.1 4.9 1.5]\n",
            " [6.3 2.3 4.4 1.3]\n",
            " [6.3 2.5 5.  1.9]\n",
            " [6.3 2.8 5.1 1.5]\n",
            " [6.3 2.7 4.9 1.8]\n",
            " [6.  3.  4.8 1.8]\n",
            " [6.  2.2 5.  1.5]\n",
            " [6.2 2.8 4.8 1.8]\n",
            " [6.5 3.  5.2 2. ]\n",
            " [7.2 3.  5.8 1.6]\n",
            " [5.6 2.8 4.9 2. ]\n",
            " [5.9 3.  5.1 1.8]\n",
            " [4.9 2.5 4.5 1.7]]\n",
            "\n",
            "Support Vector Indices for each class:\n",
            "[ 31  33  91  22  45  54  59  60  62  73  79  80 105 110   5  16  30  42\n",
            "  68  81  87 101 112 113 116]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 7:** Write a Python program to:\n",
        "\n",
        "- Load the Breast Cancer dataset\n",
        "- Train a Gaussian Naïve Bayes model\n",
        "- Print its classification report including precision, recall, and F1-score."
      ],
      "metadata": {
        "id": "5zC5UU0V6kzg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:**"
      ],
      "metadata": {
        "id": "jeAQ4tA365HO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Step 1: Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data        # Features\n",
        "y = data.target      # Target labels\n",
        "\n",
        "# Step 2: Split data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Train a Gaussian Naïve Bayes model\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Make predictions\n",
        "y_pred = gnb.predict(X_test)\n",
        "\n",
        "# Step 5: Print the classification report\n",
        "print(\"Classification Report:\\n\")\n",
        "print(classification_report(y_test, y_pred, target_names=data.target_names))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A_tnkAiw6623",
        "outputId": "2af29ec0-b570-4248-ec18-c4244ee2e363"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   malignant       1.00      0.93      0.96        43\n",
            "      benign       0.96      1.00      0.98        71\n",
            "\n",
            "    accuracy                           0.97       114\n",
            "   macro avg       0.98      0.97      0.97       114\n",
            "weighted avg       0.97      0.97      0.97       114\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 8:** Write a Python program to:\n",
        "- Train an SVM Classifier on the Wine dataset using GridSearchCV to find the best\n",
        "C and gamma.\n",
        "- Print the best hyperparameters and accuracy."
      ],
      "metadata": {
        "id": "oKfkDPFG7Ev_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:**"
      ],
      "metadata": {
        "id": "ihgPZJkk7Reg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load the Wine dataset\n",
        "data = load_wine()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Step 2: Split into training and test sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Define parameter grid for GridSearchCV\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10, 100],\n",
        "    'gamma': [0.001, 0.01, 0.1, 1]\n",
        "}\n",
        "\n",
        "# Step 4: Initialize SVM with RBF kernel\n",
        "svm = SVC(kernel='rbf')\n",
        "\n",
        "# Step 5: Perform grid search with 5-fold cross-validation\n",
        "grid_search = GridSearchCV(svm, param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Step 6: Evaluate on test set\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Step 7: Output results\n",
        "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
        "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K7-vxwND7yiB",
        "outputId": "400cfc00-c0f5-4d73-d161-441d5d444b0d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters: {'C': 100, 'gamma': 0.001}\n",
            "Test Accuracy: 83.33%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 9**: Write a Python program to:\n",
        "- Train a Naïve Bayes Classifier on a synthetic text dataset (e.g. using\n",
        "sklearn.datasets.fetch_20newsgroups).\n",
        "- Print the model's ROC-AUC score for its predictions."
      ],
      "metadata": {
        "id": "sb7MJIDT76fQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:**"
      ],
      "metadata": {
        "id": "-FvCwQQ78B5f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "# Step 1: Load a subset of the 20 Newsgroups dataset (binary classification for ROC-AUC)\n",
        "categories = ['rec.sport.baseball', 'sci.med']  # binary classes\n",
        "data = fetch_20newsgroups(subset='all', categories=categories, remove=('headers', 'footers', 'quotes'))\n",
        "\n",
        "X = data.data\n",
        "y = data.target  # Binary: 0 or 1\n",
        "\n",
        "# Step 2: Vectorize the text using TF-IDF\n",
        "vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
        "X_vec = vectorizer.fit_transform(X)\n",
        "\n",
        "# Step 3: Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_vec, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 4: Train a Multinomial Naive Bayes classifier\n",
        "model = MultinomialNB()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Get predicted probabilities and compute ROC-AUC\n",
        "y_probs = model.predict_proba(X_test)[:, 1]  # probability for class 1\n",
        "roc_auc = roc_auc_score(y_test, y_probs)\n",
        "\n",
        "# Step 6: Print ROC-AUC Score\n",
        "print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vsrBn9Kl8Gh3",
        "outputId": "2fae4d74-f134-4c18-a449-efdb65ec9158"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROC-AUC Score: 0.9971\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 10:** Imagine you’re working as a data scientist for a company that handles\n",
        "email communications.\n",
        "Your task is to automatically classify emails as Spam or Not Spam. The emails may\n",
        "contain:\n",
        "- Text with diverse vocabulary\n",
        "- Potential class imbalance (far more legitimate emails than spam)\n",
        "- Some incomplete or missing data\n",
        "\n",
        "Explain the approach you would take to:\n",
        "- Preprocess the data (e.g. text vectorization, handling missing data)\n",
        "- Choose and justify an appropriate model (SVM vs. Naïve Bayes)\n",
        "- Address class imbalance\n",
        "- Evaluate the performance of your solution with suitable metrics\n",
        "And explain the business impact of your solution."
      ],
      "metadata": {
        "id": "m5mWwJGx8RKn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:** Here's a structured approach to building an email spam classifier under real-world constraints:\n",
        "\n",
        "**1. Preprocessing the Data**\n",
        "Emails are unstructured and noisy, so preprocessing is essential.\n",
        "\n",
        "**- Text Cleaning & Normalization**\n",
        "- Lowercase conversion\n",
        "\n",
        "- Remove punctuation, numbers, and special characters\n",
        "\n",
        "- Remove HTML tags and email headers (if present)\n",
        "\n",
        "- Remove stopwords (e.g., \"the\", \"and\", \"is\")\n",
        "\n",
        "- Apply stemming or lemmatization\n",
        "\n",
        "**- Handling Missing Data**\n",
        "- Missing text: Replace with a placeholder like \"no content\" (don't drop unless completely empty).\n",
        "\n",
        "- Missing metadata (e.g., subject line): Use domain knowledge to decide whether to impute, drop, or treat as a separate feature.\n",
        "\n",
        "**- Vectorization (Feature Extraction)**\n",
        "- Use TF-IDF vectorization (Term Frequency–Inverse Document Frequency) to convert emails to numerical features while capturing word importance.\n",
        "\n",
        "- Consider adding additional features:\n",
        "\n",
        "- Number of links\n",
        "\n",
        "- Number of capital letters\n",
        "\n",
        "- Presence of spammy words (e.g., “free”, “offer”)\n",
        "\n",
        "- Email length\n",
        "\n",
        "**2. Model Selection: SVM vs. Naïve Bayes**\n",
        "\n",
        "| Aspect                             | Naïve Bayes                                  | SVM                                         |\n",
        "| ---------------------------------- | -------------------------------------------- | ------------------------------------------- |\n",
        "| **Speed**                          | Very fast                                    | Slower with large data                      |\n",
        "| **Assumptions**                    | Assumes word independence (naïve)            | No distribution assumptions                 |\n",
        "| **Works well on**                  | Text classification with clear word patterns | High-dimensional sparse data (e.g., TF-IDF) |\n",
        "| **Performance on Imbalanced Data** | Decent with class priors                     | Needs tuning and imbalance handling         |\n",
        "\n",
        "\n",
        "**Recommendation:**\n",
        "\n",
        "- Start with Multinomial Naïve Bayes for a baseline: fast, interpretable, great for word-count-based models.\n",
        "\n",
        "- Try SVM with class weighting if higher accuracy or margin-based classification is needed.\n",
        "\n",
        "- Use ensemble methods (e.g., Random Forest, XGBoost) if interpretability is less of a concern and resources allow.\n",
        "\n",
        "**3. Handling Class Imbalance**\n",
        "\n",
        "Spam datasets often have 10-20% spam, which can bias models.\n",
        "\n",
        "**Techniques:**\n",
        "\n",
        "**- Class weights:**\n",
        "\n",
        "class_weight='balanced' in SVM or tree-based models\n",
        "\n",
        "**- Resampling:**\n",
        "\n",
        "- Oversample minority class (e.g., using SMOTE)\n",
        "\n",
        "- Undersample majority class if dataset is large\n",
        "\n",
        "**- Threshold tuning:**\n",
        "\n",
        "- Adjust the decision threshold based on ROC/Precision-Recall curve.\n",
        "\n",
        "**4. Evaluation Metrics**\n",
        "\n",
        "| Metric               | Why it's important                                         |\n",
        "| -------------------- | ---------------------------------------------------------- |\n",
        "| **Precision**        | Avoids false positives (e.g., legit emails marked as spam) |\n",
        "| **Recall**           | Captures actual spam (minimize false negatives)            |\n",
        "| **F1-score**         | Balances precision & recall                                |\n",
        "| **ROC-AUC**          | Overall ability to separate classes                        |\n",
        "| **PR-AUC**           | Better for imbalanced data                                 |\n",
        "| **Confusion Matrix** | Visualizes type of errors made                             |\n",
        "\n",
        "\n",
        "**5. Business Impact**\n",
        "**- Benefits of an Accurate Spam Filter:**\n",
        "\n",
        "- Customer trust: Ensures important legitimate emails aren’t mislabeled\n",
        "\n",
        "- Security: Filters phishing or scam emails early\n",
        "\n",
        "- Efficiency: Reduces manual review of junk emails\n",
        "\n",
        "- Reduced downtime: Employees see fewer distractions\n",
        "\n",
        "- Reputation: Keeps customer-facing communication clean\n",
        "\n",
        "**Summary**\n",
        "\n",
        "| Step              | Action                                                            |\n",
        "| ----------------- | ----------------------------------------------------------------- |\n",
        "| **Preprocessing** | Clean, tokenize, use TF-IDF, handle missing data                  |\n",
        "| **Model Choice**  | Start with Naïve Bayes, upgrade to SVM if needed                  |\n",
        "| **Imbalance**     | Use class weights or resampling                                   |\n",
        "| **Evaluation**    | Focus on precision, recall, F1-score, and ROC/PR-AUC              |\n",
        "| **Impact**        | Improves productivity, protects from threats, enhances user trust |\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PbH8rJdr9uSv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Import required libraries\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import numpy as np\n",
        "\n",
        "# Step 2: Load binary classes as spam-like vs. ham-like\n",
        "categories = ['rec.sport.hockey', 'talk.politics.misc']  # Hockey = not spam, Politics = spam-like\n",
        "data = fetch_20newsgroups(subset='all', categories=categories, remove=('headers', 'footers', 'quotes'))\n",
        "\n",
        "X_raw = data.data\n",
        "y = data.target  # 0 and 1\n",
        "\n",
        "# Step 3: Handle missing data\n",
        "X = ['no content' if x.strip() == '' else x for x in X_raw]\n",
        "\n",
        "# Step 4: Text vectorization using TF-IDF\n",
        "vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
        "X_vec = vectorizer.fit_transform(X)\n",
        "\n",
        "# Step 5: Split dataset into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_vec, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# ----------- Naïve Bayes Model ----------- #\n",
        "print(\"=== Naïve Bayes Classifier ===\")\n",
        "nb_model = MultinomialNB()\n",
        "nb_model.fit(X_train, y_train)\n",
        "nb_preds = nb_model.predict(X_test)\n",
        "nb_probs = nb_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Evaluation\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, nb_preds))\n",
        "print(\"ROC-AUC Score:\", roc_auc_score(y_test, nb_probs))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, nb_preds))\n",
        "\n",
        "# ----------- SVM Model (with class weight handling) ----------- #\n",
        "print(\"\\n=== SVM Classifier with Class Weight ===\")\n",
        "svm_model = SVC(kernel='linear', probability=True, class_weight='balanced', random_state=42)\n",
        "svm_model.fit(X_train, y_train)\n",
        "svm_preds = svm_model.predict(X_test)\n",
        "svm_probs = svm_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Evaluation\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, svm_preds))\n",
        "print(\"ROC-AUC Score:\", roc_auc_score(y_test, svm_probs))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, svm_preds))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vpsHVGww_rU-",
        "outputId": "3d3b2b12-95f8-4fd8-c6d7-30b8d39183a1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Naïve Bayes Classifier ===\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.96      0.95       200\n",
            "           1       0.95      0.92      0.94       155\n",
            "\n",
            "    accuracy                           0.95       355\n",
            "   macro avg       0.95      0.94      0.95       355\n",
            "weighted avg       0.95      0.95      0.95       355\n",
            "\n",
            "ROC-AUC Score: 0.9924677419354838\n",
            "Confusion Matrix:\n",
            " [[193   7]\n",
            " [ 12 143]]\n",
            "\n",
            "=== SVM Classifier with Class Weight ===\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.92      0.94       200\n",
            "           1       0.90      0.95      0.92       155\n",
            "\n",
            "    accuracy                           0.93       355\n",
            "   macro avg       0.93      0.93      0.93       355\n",
            "weighted avg       0.93      0.93      0.93       355\n",
            "\n",
            "ROC-AUC Score: 0.9742741935483871\n",
            "Confusion Matrix:\n",
            " [[183  17]\n",
            " [  8 147]]\n"
          ]
        }
      ]
    }
  ]
}