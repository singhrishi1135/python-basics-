{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1:** What is Ensemble Learning in machine learning? Explain the key idea behind it.\n"
      ],
      "metadata": {
        "id": "AvdK_suIjngq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:** Ensemble Learning is a machine learning technique that combines multiple models (often called \"base learners\" or \"weak learners\") to produce a more powerful and accurate predictive model than any individual model alone.\n",
        "\n",
        "**Key Idea Behind Ensemble Learning:**\n",
        "\n",
        "\"A group of weak learners can come together to form a strong learner.\"\n",
        "\n",
        "Use of Ensemble Learning:-\n",
        "- Reduces variance (avoids overfitting)\n",
        "\n",
        "- Reduces bias (improves underfitting)\n",
        "\n",
        "- Improves accuracy and generalization\n",
        "\n",
        "- Increases stability of predictions\n",
        "\n",
        "**Common Types of Ensemble Methods:**\n",
        "**1.Bagging (Bootstrap Aggregating):**\n",
        "\n",
        "- Trains multiple models independently on random subsets of data (with replacement).\n",
        "\n",
        "- Reduces variance.\n",
        "\n",
        "- Example: Random Forest\n",
        "\n",
        "**2. Boosting:**\n",
        "\n",
        "- Trains models sequentially, where each model tries to correct the errors of the previous one.\n",
        "\n",
        "- Reduces bias.\n",
        "\n",
        "- Examples: AdaBoost, Gradient Boosting, XGBoost\n",
        "\n",
        "**3. Stacking:**\n",
        "\n",
        "- Combines predictions of several base models using a meta-model (second-level learner).\n",
        "\n",
        "- Uses predictions of base learners as input features for the final model."
      ],
      "metadata": {
        "id": "XJLfYZajjs-Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 2:** What is the difference between Bagging and Boosting?"
      ],
      "metadata": {
        "id": "Aig4lRAplZBh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:**\n",
        "\n",
        "Difference between Bagging and Boosting:-\n",
        "\n",
        "| Feature                 | **Bagging**                                     | **Boosting**                                       |\n",
        "| ----------------------- | ----------------------------------------------- | -------------------------------------------------- |\n",
        "| **Full Form**           | Bootstrap Aggregating                           | —                                                  |\n",
        "| **Purpose**             | Reduce **variance**                             | Reduce **bias** and **variance**                   |\n",
        "| **Model Training**      | Models trained **in parallel**                  | Models trained **sequentially**                    |\n",
        "| **Data Sampling**       | Random subsets with replacement (bootstrapping) | Each new model focuses more on **previous errors** |\n",
        "| **Model Weighting**     | Equal weight to all models                      | Models are **weighted** based on performance       |\n",
        "| **Error Handling**      | Averages out predictions to reduce overfitting  | Learns from mistakes; focuses on hard cases        |\n",
        "| **Risk of Overfitting** | Lower                                           | Higher (especially if not regularized)             |\n",
        "| **Examples**            | Random Forest                                   | AdaBoost, Gradient Boosting, XGBoost, LightGBM     |\n",
        "\n",
        "\n",
        "**Bagging:** Like asking 100 people the same question independently and taking the majority vote — reduces randomness.\n",
        "\n",
        "**Boosting:** Like asking a tutor to explain a topic repeatedly, each time focusing more on what you didn't understand — improves performance step-by-step.\n",
        "\n",
        "Example:\n",
        "\n",
        "- Use Bagging (like Random Forest) when the model is high variance (e.g., decision trees).\n",
        "\n",
        "- Use Boosting when the model needs to learn complex patterns and improve bias (e.g., weak learners).\n"
      ],
      "metadata": {
        "id": "TFAGmhRWlah2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 3:** What is bootstrap sampling and what role does it play in Bagging methods\n",
        "like Random Forest?"
      ],
      "metadata": {
        "id": "3fMbpVstl4EX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:** Bootstrap Sampling is a statistical technique where multiple random samples are drawn from a dataset with replacement.\n",
        "This means the same data point can appear multiple times in a single sample, and some may not appear at all.\n",
        "\n",
        "*Key Properties of Bootstrap Sampling:*\n",
        "\n",
        "- Sample size is usually equal to the original dataset size.\n",
        "\n",
        "- Each sample is random, and some instances are repeated.\n",
        "\n",
        "- Enables estimation of model uncertainty and variance.\n",
        "\n",
        "**Role of Bootstrap Sampling in Bagging (e.g., Random Forest):**\n",
        "In Bagging, especially in Random Forest, bootstrap sampling is used to create diverse training datasets for each base model (typically decision trees).\n",
        "\n",
        "**Here's how it works:**\n",
        "- From the original dataset, generate N bootstrap samples.\n",
        "\n",
        "- Train a separate model (like a decision tree) on each bootstrap sample.\n",
        "\n",
        "** Aggregate the predictions of all models:**\n",
        "\n",
        "- Classification: majority vote\n",
        "\n",
        "- Regression: average output\n",
        "\n",
        "**In Random Forest specifically:**\n",
        "- Bootstrap sampling ensures that each tree is different, even if built on the same original data.\n",
        "\n",
        "- Combined with feature randomness (each split considers a random subset of features), this adds diversity and helps reduce overfitting.\n",
        "\n",
        "- Overall, this results in a more stable and generalizable ensemble model."
      ],
      "metadata": {
        "id": "GSlXkTEal5vG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 4:** What are Out-of-Bag (OOB) samples and how is OOB score used to\n",
        "evaluate ensemble models?\n"
      ],
      "metadata": {
        "id": "gw4345mUmdkv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:** In bootstrap sampling, each model (e.g., a decision tree in Random Forest) is trained on a random subset of the data with replacement. As a result, around 63% of the original data points typically appear in each bootstrap sample, leaving about 37% of the data unused for that model.\n",
        "\n",
        "These unused data points are called:\n",
        "\n",
        "***Out-of-Bag (OOB) samples***\n",
        "\n",
        "**Purpose of OOB Samples:**\n",
        "OOB samples serve as a validation set for each corresponding model — without needing a separate validation set.\n",
        "\n",
        "**OOB Score**\n",
        "The OOB score is an internal performance estimate of a bagging model (like Random Forest) based on predictions made on the OOB samples.\n",
        "\n",
        "**How it's calculated:**\n",
        "\n",
        "1. For each data point in the dataset:\n",
        "\n",
        "- Find all the trees where it was OOB (not used in training).\n",
        "\n",
        "- Predict using only those trees.\n",
        "\n",
        "2. Compare the aggregated OOB predictions to the true labels.\n",
        "\n",
        "3. Compute accuracy (for classification) or R² score (for regression)."
      ],
      "metadata": {
        "id": "K53TYvS7melw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 5:** Compare feature importance analysis in a single Decision Tree vs. a Random Forest."
      ],
      "metadata": {
        "id": "o7KFdu5D4EXr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:** Feature Importance: Decision Tree vs. Random Forest\n",
        "Feature importance helps us understand which features (columns) are most influential in making predictions.\n",
        "\n",
        "**1. In a Single Decision Tree:**\n",
        "- Feature importance is calculated based on how much a feature reduces impurity (e.g., Gini or entropy) at each split.\n",
        "\n",
        "- If a feature is used closer to the root node, it usually has higher importance.\n",
        "\n",
        "- It is simple to interpret but also sensitive to noise and data variations.\n",
        "\n",
        "**2. In a Random Forest:**\n",
        "- Feature importance is aggregated over many trees.\n",
        "\n",
        "For each tree:\n",
        "\n",
        "- It computes the impurity reduction caused by each feature.\n",
        "\n",
        "- The final importance is the average across all trees.\n",
        "\n",
        "- More stable and reliable than a single tree, especially on noisy data.\n",
        "\n",
        "**Comparison Table:**\n",
        "\n",
        "| Aspect                  | **Decision Tree**                        | **Random Forest**                            |\n",
        "| ----------------------- | ---------------------------------------- | -------------------------------------------- |\n",
        "| **Stability**           | Less stable, sensitive to data variation | More stable, due to averaging across trees   |\n",
        "| **Interpretability**    | Easier to interpret                      | Harder, but more reliable                    |\n",
        "| **Bias/Variance**       | Higher variance                          | Lower variance                               |\n",
        "| **Robustness to Noise** | Less robust                              | More robust                                  |\n",
        "| **Use in Practice**     | Good for quick insights                  | Preferred for more accurate feature rankings |\n"
      ],
      "metadata": {
        "id": "S2GAEiJm4TSt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 6:** Write a Python program to:\n",
        "- Load the Breast Cancer dataset using\n",
        "sklearn.datasets.load_breast_cancer()\n",
        "- Train a Random Forest Classifier\n",
        "- Print the top 5 most important features based on feature importance scores.\n"
      ],
      "metadata": {
        "id": "mDUdYnnf408z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:**"
      ],
      "metadata": {
        "id": "QWBf5zsv5FzN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# Train Random Forest Classifier\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X, y)\n",
        "\n",
        "# Get feature importances\n",
        "importances = rf.feature_importances_\n",
        "\n",
        "# Create a DataFrame for better visualization\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Importance': importances\n",
        "})\n",
        "\n",
        "# Sort by importance and print top 5\n",
        "top_features = feature_importance_df.sort_values(by='Importance', ascending=False).head(5)\n",
        "print(\"Top 5 Most Important Features:\")\n",
        "print(top_features)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6lZrZirX5MCr",
        "outputId": "814f1a66-f4ef-452c-88fc-9f492053d3cc"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 Most Important Features:\n",
            "                 Feature  Importance\n",
            "23            worst area    0.139357\n",
            "27  worst concave points    0.132225\n",
            "7    mean concave points    0.107046\n",
            "20          worst radius    0.082848\n",
            "22       worst perimeter    0.080850\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 7:** Write a Python program to:\n",
        "- Train a Bagging Classifier using Decision Trees on the Iris dataset\n",
        "- Evaluate its accuracy and compare with a single Decision Tree"
      ],
      "metadata": {
        "id": "x0FqlhbB5QJM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:**"
      ],
      "metadata": {
        "id": "VhYTf9xl5gn8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "import sklearn\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Train a single Decision Tree\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "dt.fit(X_train, y_train)\n",
        "dt_preds = dt.predict(X_test)\n",
        "dt_accuracy = accuracy_score(y_test, dt_preds)\n",
        "\n",
        "# Train a Bagging Classifier using Decision Trees\n",
        "# Use 'estimator' if sklearn version >= 1.2, else use 'base_estimator'\n",
        "if sklearn.__version__ >= \"1.2\":\n",
        "    bagging = BaggingClassifier(\n",
        "        estimator=DecisionTreeClassifier(),\n",
        "        n_estimators=50,\n",
        "        random_state=42\n",
        "    )\n",
        "else:\n",
        "    bagging = BaggingClassifier(\n",
        "        base_estimator=DecisionTreeClassifier(),\n",
        "        n_estimators=50,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "bagging.fit(X_train, y_train)\n",
        "bagging_preds = bagging.predict(X_test)\n",
        "bagging_accuracy = accuracy_score(y_test, bagging_preds)\n",
        "\n",
        "# Print results\n",
        "print(f\"Accuracy of Single Decision Tree: {dt_accuracy:.4f}\")\n",
        "print(f\"Accuracy of Bagging Classifier  : {bagging_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D-ABwW1S6IJu",
        "outputId": "efbc695e-555a-4fed-eceb-5044ff56f05b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Single Decision Tree: 1.0000\n",
            "Accuracy of Bagging Classifier  : 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 8:** Write a Python program to:\n",
        "- Train a Random Forest Classifier\n",
        "- Tune hyperparameters max_depth and n_estimators using GridSearchCV\n",
        "- Print the best parameters and final accuracy\n"
      ],
      "metadata": {
        "id": "9GBeStlp6Pos"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:**"
      ],
      "metadata": {
        "id": "Zps0YIS27mk7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Define Random Forest model\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Define hyperparameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [10, 50, 100],\n",
        "    'max_depth': [None, 3, 5, 7]\n",
        "}\n",
        "\n",
        "# Set up GridSearchCV\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=rf,\n",
        "    param_grid=param_grid,\n",
        "    cv=5,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Fit the model with grid search\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best model and predictions\n",
        "best_rf = grid_search.best_estimator_\n",
        "y_pred = best_rf.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print results\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(f\"Final Accuracy on Test Set: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXAGv25r7opV",
        "outputId": "eb2f55ef-1ead-452c-b39c-0d15badb31ba"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': None, 'n_estimators': 100}\n",
            "Final Accuracy on Test Set: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 9:** Write a Python program to:\n",
        "- Train a Bagging Regressor and a Random Forest Regressor on the California\n",
        "Housing dataset\n",
        "- Compare their Mean Squared Errors (MSE)"
      ],
      "metadata": {
        "id": "r0bXpblo7uw1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:**"
      ],
      "metadata": {
        "id": "xKpA3t-h8649"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "# Load the California Housing dataset\n",
        "data = fetch_california_housing()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Train a Bagging Regressor (using Decision Trees as base estimator)\n",
        "bagging_regressor = BaggingRegressor(\n",
        "    estimator=DecisionTreeRegressor(),\n",
        "    n_estimators=50,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "bagging_regressor.fit(X_train, y_train)\n",
        "bagging_preds = bagging_regressor.predict(X_test)\n",
        "\n",
        "# Train a Random Forest Regressor\n",
        "rf_regressor = RandomForestRegressor(\n",
        "    n_estimators=100,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "rf_regressor.fit(X_train, y_train)\n",
        "rf_preds = rf_regressor.predict(X_test)\n",
        "\n",
        "# Calculate Mean Squared Errors\n",
        "bagging_mse = mean_squared_error(y_test, bagging_preds)\n",
        "rf_mse = mean_squared_error(y_test, rf_preds)\n",
        "\n",
        "# Print results\n",
        "print(f\"Bagging Regressor MSE:       {bagging_mse:.4f}\")\n",
        "print(f\"Random Forest Regressor MSE: {rf_mse:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WrXMi33y9IJ8",
        "outputId": "39d5de59-a738-4c29-e780-a914b6dc6d89"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging Regressor MSE:       0.2579\n",
            "Random Forest Regressor MSE: 0.2565\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 10:** You are working as a data scientist at a financial institution to predict loan\n",
        "default. You have access to customer demographic and transaction history data.\n",
        "You decide to use ensemble techniques to increase model performance.\n",
        "Explain your step-by-step approach to:\n",
        "- Choose between Bagging or Boosting\n",
        "- Handle overfitting\n",
        "- Select base models\n",
        "- Evaluate performance using cross-validation\n",
        "- Justify how ensemble learning improves decision-making in this real-world\n",
        "context.\n"
      ],
      "metadata": {
        "id": "0AzOq79L9O2b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:**\n",
        "Predict loan default (a binary classification task):\n",
        "\n",
        "| Technique                              | When to Use                                                                      | Recommendation                                                                  |\n",
        "| -------------------------------------- | -------------------------------------------------------------------------------- | ------------------------------------------------------------------------------- |\n",
        "| **Bagging** (e.g., Random Forest)      | When the model has **high variance**, prone to **overfitting**                   | Good for **baseline models** and robust predictions                             |\n",
        "| **Boosting** (e.g., XGBoost, LightGBM) | When the model has **high bias**, and needs to capture **complex relationships** | Better for **fine-grained patterns** and optimizing **minor class performance** |\n",
        "\n",
        "\n",
        "**Handle Overfitting**\n",
        "\n",
        "Overfitting is a risk, especially with Boosting. Use the following:\n",
        "\n",
        "##Strategies:\n",
        "**Feature selection:** Remove irrelevant or highly correlated features.\n",
        "\n",
        "**Regularization:**\n",
        "\n",
        "- For Random Forest: limit max_depth, use min_samples_split, max_features\n",
        "\n",
        "- For XGBoost: use reg_alpha, reg_lambda, and control learning_rate\n",
        "\n",
        "**Cross-validation:** Detect overfitting early\n",
        "\n",
        "- Early stopping (Boosting): Stop training when performance stops improving\n",
        "\n",
        "- Dropout-like techniques: Use stochastic depth or row sampling\n",
        "\n",
        "**Select Base Models**\n",
        "\n",
        "- Decision Tree: Default choice for both Bagging and Boosting\n",
        "\n",
        "- Logistic Regression: If you want to try stacking or simpler models\n",
        "\n",
        "- KNN/SVM: Generally not preferred as base learners for ensembles\n",
        "\n",
        "**Evaluate Performance Using Cross-Validation**\n",
        "\n",
        "**Evaluation Metrics:**\n",
        "- Accuracy (overall)\n",
        "\n",
        "- Precision/Recall/F1-score (important in imbalanced data)\n",
        "\n",
        "- AUC-ROC (measures model's ability to rank defaults higher than non-defaults)\n",
        "\n",
        "- Confusion matrix (to analyze false positives/negatives)\n",
        "\n",
        "**Justify Ensemble Learning in Real-World Decision-Making**\n",
        "\n",
        "**Higher accuracy** → fewer misclassifications → better loan portfolio management\n",
        "\n",
        "**Reduces variance and bias** → more stable predictions in production\n",
        "\n",
        "**Handles class imbalance** better (especially boosting techniques)\n",
        "\n",
        "**Improves trust** with feature importance/explainability (e.g., SHAP for XGBoost)\n",
        "\n",
        "**Mitigates risk:** Correctly identifying defaulters reduces financial loss\n",
        "\n",
        "**Supports regulation:** Transparent models with consistent results aid in compliance."
      ],
      "metadata": {
        "id": "f3L_TPMF95PV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score, train_test_split\n",
        "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix\n",
        "\n",
        "# Simulate loan default dataset (for illustration)\n",
        "# In real projects, load your data with pd.read_csv()\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_informative=10,\n",
        "                           n_redundant=5, n_classes=2, weights=[0.7, 0.3],\n",
        "                           random_state=42)\n",
        "\n",
        "# Split into train/test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y,\n",
        "                                                    test_size=0.3, random_state=42)\n",
        "\n",
        "# ----------------------- Model 1: Random Forest (Bagging) -----------------------\n",
        "rf = RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    max_depth=7,\n",
        "    class_weight='balanced',  # handle imbalance\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Cross-validation (5-fold)\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "rf_scores = cross_val_score(rf, X_train, y_train, cv=cv, scoring='roc_auc')\n",
        "\n",
        "# Train final model\n",
        "rf.fit(X_train, y_train)\n",
        "rf_preds = rf.predict(X_test)\n",
        "rf_proba = rf.predict_proba(X_test)[:, 1]\n",
        "rf_auc = roc_auc_score(y_test, rf_proba)\n",
        "\n",
        "print(\"=== Random Forest Results ===\")\n",
        "print(\"Cross-validated AUC scores:\", rf_scores)\n",
        "print(\"Mean CV AUC:\", rf_scores.mean())\n",
        "print(\"Test AUC:\", rf_auc)\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, rf_preds))\n",
        "print(classification_report(y_test, rf_preds))\n",
        "\n",
        "# ----------------------- Model 2: XGBoost (Boosting) -----------------------\n",
        "xgb = XGBClassifier(\n",
        "    n_estimators=100,\n",
        "    max_depth=5,\n",
        "    learning_rate=0.1,\n",
        "    scale_pos_weight=2.5,  # to handle class imbalance\n",
        "    use_label_encoder=False,\n",
        "    eval_metric='logloss',\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "xgb_scores = cross_val_score(xgb, X_train, y_train, cv=cv, scoring='roc_auc')\n",
        "\n",
        "xgb.fit(X_train, y_train)\n",
        "xgb_preds = xgb.predict(X_test)\n",
        "xgb_proba = xgb.predict_proba(X_test)[:, 1]\n",
        "xgb_auc = roc_auc_score(y_test, xgb_proba)\n",
        "\n",
        "print(\"\\n=== XGBoost Results ===\")\n",
        "print(\"Cross-validated AUC scores:\", xgb_scores)\n",
        "print(\"Mean CV AUC:\", xgb_scores.mean())\n",
        "print(\"Test AUC:\", xgb_auc)\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, xgb_preds))\n",
        "print(classification_report(y_test, xgb_preds))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jYkZ8_Pq_fzV",
        "outputId": "30c964b2-8830-409b-f2ea-7148e8147cb1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Random Forest Results ===\n",
            "Cross-validated AUC scores: [0.98489571 0.97170942 0.98273795 0.92954325 0.98275024]\n",
            "Mean CV AUC: 0.970327314403516\n",
            "Test AUC: 0.9541511120458489\n",
            "Confusion Matrix:\n",
            " [[207   2]\n",
            " [ 23  68]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.99      0.94       209\n",
            "           1       0.97      0.75      0.84        91\n",
            "\n",
            "    accuracy                           0.92       300\n",
            "   macro avg       0.94      0.87      0.89       300\n",
            "weighted avg       0.92      0.92      0.91       300\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [16:51:04] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [16:51:04] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [16:51:04] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [16:51:04] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [16:51:04] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== XGBoost Results ===\n",
            "Cross-validated AUC scores: [0.99232798 0.97914169 0.99064972 0.91909621 0.98226433]\n",
            "Mean CV AUC: 0.9726959880092052\n",
            "Test AUC: 0.9688206530311794\n",
            "Confusion Matrix:\n",
            " [[206   3]\n",
            " [ 12  79]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.99      0.96       209\n",
            "           1       0.96      0.87      0.91        91\n",
            "\n",
            "    accuracy                           0.95       300\n",
            "   macro avg       0.95      0.93      0.94       300\n",
            "weighted avg       0.95      0.95      0.95       300\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [16:51:05] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        }
      ]
    }
  ]
}